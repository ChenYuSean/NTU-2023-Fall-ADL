{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":92553,"status":"ok","timestamp":1700549917444,"user":{"displayName":"陳裕翔","userId":"07684278669663689814"},"user_tz":-480},"id":"_19JRFfog7pB","outputId":"85fc1cfc-7799-4cc4-c007-167f495d0e5b"},"outputs":[],"source":["# !pip install torch==2.1.0\n","# !pip install transformers==4.34.1\n","# !pip install bitsandbytes\n","# !pip install peft==0.6.0\n","# !pip install datasets\n","# !pip install evaluate\n","# !pip install accelerate\n","# !pip install sentencepiece\n","# !pip install einops\n","# !pip install scikit-learn\n","# !pip install ipdb"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"l8_KLgrMiRZ9"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/vr/anaconda3/envs/adl-hw3-copy/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from collections import defaultdict\n","import copy\n","import json\n","import os\n","from os.path import exists, join, isdir\n","from dataclasses import dataclass, field\n","import sys\n","from typing import Optional, Dict, Sequence\n","import numpy as np\n","from tqdm import tqdm\n","import logging\n","import bitsandbytes as bnb\n","import pandas as pd\n","import importlib\n","\n","import torch\n","import transformers\n","from torch.nn.utils.rnn import pad_sequence\n","import argparse\n","from transformers import (\n","    set_seed,\n","    AutoTokenizer,\n","    AutoConfig,\n","    AutoModelForCausalLM,\n","    Seq2SeqTrainer,\n","    BitsAndBytesConfig,\n","    LlamaTokenizer\n","\n",")\n","from datasets import load_dataset, Dataset\n","from peft import (\n","    prepare_model_for_kbit_training,\n","    LoraConfig,\n","    get_peft_model,\n","    PeftModel\n",")\n","from peft.tuners.lora import LoraLayer\n","from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n","\n","from accelerate import notebook_launcher\n","from accelerate import Accelerator\n","from torch.utils.data import DataLoader"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Hc-vaBmBiXL9"},"outputs":[],"source":["# Global variables\n","FROM_COLAB = False\n","DEBUG = False\n","ROOT_PATH = './'\n","str_args = None"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16753,"status":"ok","timestamp":1700549970794,"user":{"displayName":"陳裕翔","userId":"07684278669663689814"},"user_tz":-480},"id":"Cc19BSIoiXpV","outputId":"ea7b0079-1702-4e54-c4f5-8c5df2c52b55"},"outputs":[],"source":["if FROM_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    ROOT_PATH = 'drive/MyDrive/Colab Notebooks/ADL/HW3/'\n","if DEBUG:\n","    import ipdb"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"TG9D4dB-iZZN"},"outputs":[],"source":["# Comment out when using .py file\n","str_args = [\n","    \"--test_file\", ROOT_PATH + \"data/private_test.json\",\n","    \"--model_name_or_path\", ROOT_PATH + \"Taiwan-LLM-7B-v2.0-chat\",\n","    \"--peft_model\", ROOT_PATH + \"OUTPUTS/300steps/checkpoint-250\",\n","    \"--batch_size\", \"2\",\n","    \"--output_path\", \"./prediction.json\"\n","]"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"9GD7A5LIlFb-"},"outputs":[],"source":["# Parser\n","def parse_args(str_args = None):\n","    parser = argparse.ArgumentParser()\n","    # Data\n","    parser.add_argument(\"--seed\", type=int, default=None)\n","    parser.add_argument(\"--test_file\", type=str ,required=True)\n","    parser.add_argument(\n","        \"--output_path\",\n","        type=str,\n","        default=\"./prediction.json\"\n","    )\n","    parser.add_argument(\n","        \"--model_name_or_path\",\n","        type=str,\n","        default = None\n","    )\n","    parser.add_argument(\n","        \"--peft_model\",\n","        type=str,\n","        default = None\n","    )\n","    # Trainer Parameters\n","    parser.add_argument(\n","        \"--batch_size\",\n","        type=int,\n","        default=1,\n","    )\n","    parser.add_argument(\n","        \"--source_max_len\",\n","        type=int,\n","        default=1024,\n","    )\n","    # Generation Argument\n","    parser.add_argument(\n","        \"--max_new_tokens\",\n","        type=int,\n","        default=256\n","    )\n","    parser.add_argument(\n","        \"--min_new_tokens\",\n","        type=int,\n","        default=None\n","    )\n","    parser.add_argument(\n","        \"--do_sample\",\n","        action='store_true'\n","    )\n","    parser.add_argument(\n","        \"--num_beams\",\n","        type=int,\n","        default=1\n","    )\n","    parser.add_argument(\n","        \"--num_beam_groups\",\n","        type=int,\n","        default=1\n","    )\n","\n","    args = parser.parse_args(str_args)\n","    return args"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"C-u-ZOGlyzOP"},"outputs":[],"source":["@dataclass\n","class DataCollatorForCausalLM(object):\n","    tokenizer: transformers.PreTrainedTokenizer\n","    source_max_len: int\n","\n","    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n","        # Extract elements\n","        IGNORE_INDEX = -100\n","        sources = [f\"{self.tokenizer.bos_token}{example['input']}\" for example in instances]\n","        # Tokenize\n","        tokenized_sources_with_prompt = self.tokenizer(\n","            sources,\n","            max_length=self.source_max_len,\n","            truncation=True,\n","            add_special_tokens=False,\n","        )\n","        # Build the input for causal LM\n","        input_ids = []\n","        for tokenized_source in tokenized_sources_with_prompt['input_ids']:\n","                input_ids.append(torch.tensor(tokenized_source))\n","        # Apply padding\n","        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n","        data_dict = {\n","            'input_ids': input_ids,\n","            'attention_mask':input_ids.ne(self.tokenizer.pad_token_id),\n","        }\n","        return data_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_prompt(instruction: str) -> str:\n","    '''Format the instruction as a prompt for LLM.'''\n","    return f\"你是人工智慧助理，以下是用戶和人工智能助理之間的對話。你要對用戶的問題提供有用、安全、詳細和禮貌的回答。USER: {instruction} ASSISTANT:\"\n","\n","\n","def get_bnb_config() -> BitsAndBytesConfig:\n","    '''Get the BitsAndBytesConfig.'''\n","    bnb_config = BitsAndBytesConfig(\n","            load_in_4bit= True,\n","            llm_int8_threshold=6.0,\n","            llm_int8_has_fp16_weight=False,\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_use_double_quant=False,\n","            bnb_4bit_quant_type=\"nf4\"\n","    )\n","    return bnb_config\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"fJbQxjbOzLLG"},"outputs":[],"source":["def main(str_args = None):\n","    args = parse_args(str_args)\n","\n","args = parse_args(str_args)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"2bIBeA_SyT_f"},"outputs":[],"source":["# Prepare\n","logger = logging.getLogger(__name__)\n","compute_dtype = torch.float16\n","if args.seed is not None:\n","    set_seed(args.seed)\n","if args.output_path is not None:\n","    output_dir = os.path.join(*args.output_path.split(\"/\")[:-1])\n","    os.makedirs(output_dir, exist_ok=True)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"avv5Xjdxzin_"},"outputs":[{"name":"stdout","output_type":"stream","text":["Load Dataset\n"]},{"name":"stderr","output_type":"stream","text":["Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 6636.56it/s]\n","Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1038.71it/s]\n","Generating train split: 250 examples [00:00, 63596.31 examples/s]\n","Map: 100%|██████████| 250/250 [00:00<00:00, 21019.87 examples/s]\n"]}],"source":["# Load dataset\n","print('Load Dataset')\n","def format_dataset(dataset):\n","    def processing(example):\n","        return {'input': get_prompt(example['instruction'])}\n","    dataset = dataset.map(processing)\n","    # Remove \n","    dataset = dataset.remove_columns(\n","        [col for col in dataset.column_names if col in ['instruction','output']]\n","    )\n","    return dataset\n","\n","raw_dataset = load_dataset(\"json\", data_files=args.test_file,split='train')\n","test_dataset = format_dataset(raw_dataset)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"d-26dPvMpPiz"},"outputs":[{"name":"stdout","output_type":"stream","text":["Load Model\n"]},{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.34s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Loading adapters.\n"]}],"source":["# Load Model\n","print('Load Model')\n","bnb_config = get_bnb_config()\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    args.model_name_or_path,\n","    quantization_config = bnb_config,\n","    load_in_4bit = True,\n","    torch_dtype=compute_dtype,\n","    device_map = 'cuda:0'\n",")\n","base_model.config.torch_dtype=compute_dtype\n","# Load PeftModel\n","print(\"Loading adapters.\")\n","model = PeftModel.from_pretrained(base_model, args.peft_model)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"XCV0lOsF0pa3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Load Tokenizer\n"]},{"data":{"text/plain":["0"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Load Tokenizer\n","print('Load Tokenizer')\n","tokenizer = AutoTokenizer.from_pretrained(\n","    args.model_name_or_path,\n","    padding_side=\"right\",\n","    use_fast=False,\n","    tokenizer_type='llama'\n",")\n","tokenizer.add_special_tokens({\n","    \"eos_token\": tokenizer.convert_ids_to_tokens(base_model.config.eos_token_id),\n","    \"bos_token\": tokenizer.convert_ids_to_tokens(base_model.config.bos_token_id),\n","    \"unk_token\": tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id),\n","})"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"gctkZeYl1Dip"},"outputs":[],"source":["# Data Collator\n","data_collator = DataCollatorForCausalLM(\n","    tokenizer=tokenizer,\n","    source_max_len=args.source_max_len\n",")\n","# Generatrion Config\n","gen_config = transformers.GenerationConfig(\n","    max_new_tokens = args.max_new_tokens,\n","    min_new_tokens = args.min_new_tokens,\n","    do_sample = args.do_sample,\n","    num_beams = args.num_beams,\n","    num_beam_groups = args.num_beam_groups,\n","    )\n"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"hhfJi7Fr2cxn"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/125 [00:00<?, ?it/s]/home/vr/anaconda3/envs/adl-hw3-copy/lib/python3.10/site-packages/transformers/generation/utils.py:1539: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n","  warnings.warn(\n","100%|██████████| 125/125 [12:39<00:00,  4.72s/it]"]}],"source":["test_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=args.batch_size)\n","progress = tqdm(total=len(test_dataloader))\n","model.eval()\n","all_predictions=[]\n","for step, batch in enumerate(test_dataloader):\n","    with torch.no_grad():\n","        predictions = model.generate(\n","            input_ids=batch[\"input_ids\"],\n","            attention_mask=batch[\"attention_mask\"],\n","            generation_config = gen_config,\n","        )\n","        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n","        predictions = tokenizer.batch_decode(\n","            predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n","        )\n","        all_predictions += predictions\n","        progress.update()\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["with open(args.output_path, 'w') as fout:\n","    outputs = []\n","    for i, example in enumerate(test_dataset):\n","        output_example = {}\n","        output_example['id'] = example['id'] \n","        output_example['output'] = all_predictions[i].replace(example['input'], '').strip()\n","        outputs.append(output_example)\n","    fout.write(json.dumps(outputs,indent=4,ensure_ascii=False))"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNlr2j/ArHRqpE4KyIfpPQQ","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
