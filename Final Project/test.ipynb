{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":92553,"status":"ok","timestamp":1700549917444,"user":{"displayName":"陳裕翔","userId":"07684278669663689814"},"user_tz":-480},"id":"_19JRFfog7pB","outputId":"85fc1cfc-7799-4cc4-c007-167f495d0e5b"},"outputs":[],"source":["# !pip install torch==2.1.0\n","# !pip install transformers==4.34.1\n","# !pip install bitsandbytes\n","# !pip install peft==0.6.0\n","# !pip install datasets\n","# !pip install evaluate\n","# !pip install accelerate\n","# !pip install sentencepiece\n","# !pip install einops\n","# !pip install scikit-learn\n","# !pip install ipdb"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"l8_KLgrMiRZ9"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/vr/anaconda3/envs/adl-hw3-copy/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from collections import defaultdict\n","import copy\n","import json\n","import os\n","from os.path import exists, join, isdir\n","from dataclasses import dataclass, field\n","import sys\n","from typing import Optional, Dict, Sequence\n","import numpy as np\n","from tqdm import tqdm\n","import logging\n","import bitsandbytes as bnb\n","import pandas as pd\n","import importlib\n","\n","import torch\n","import transformers\n","from torch.nn.utils.rnn import pad_sequence\n","import argparse\n","from transformers import (\n","    set_seed,\n","    AutoTokenizer,\n","    AutoConfig,\n","    AutoModelForCausalLM,\n","    Seq2SeqTrainer,\n","    BitsAndBytesConfig,\n","    LlamaTokenizer\n","\n",")\n","from datasets import load_dataset, Dataset\n","from peft import (\n","    prepare_model_for_kbit_training,\n","    LoraConfig,\n","    get_peft_model,\n","    PeftModel\n",")\n","from peft.tuners.lora import LoraLayer\n","from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n","\n","from accelerate import notebook_launcher\n","from accelerate import Accelerator\n","from torch.utils.data import DataLoader\n","from data_process import get_prompt, prepare_dataset"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"Hc-vaBmBiXL9"},"outputs":[],"source":["# Global variables\n","FROM_COLAB = False\n","DEBUG = False\n","ROOT_PATH = './'\n","str_args = None"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16753,"status":"ok","timestamp":1700549970794,"user":{"displayName":"陳裕翔","userId":"07684278669663689814"},"user_tz":-480},"id":"Cc19BSIoiXpV","outputId":"ea7b0079-1702-4e54-c4f5-8c5df2c52b55"},"outputs":[],"source":["if FROM_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    ROOT_PATH = 'drive/MyDrive/Colab Notebooks/ADL/HW3/'\n","if DEBUG:\n","    import ipdb"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"TG9D4dB-iZZN"},"outputs":[],"source":["# Comment out when using .py file\n","str_args = [\n","    \"--test_file\", \"eval_data/-5lKAOBHZgs.json\",\n","    \"--model_name_or_path\", \"/home/vr/disk/YuSean/ADL/HW3/Taiwan-LLM-7B-v2.0-chat\",\n","    \"--peft_model\", \"output/checkpoint-100\",\n","    \"--test_size\", \"50\",\n","    \"--batch_size\", \"4\",\n","    \"--output_path\", \"./prediction.json\",\n","    \"--do_sample\",\n","    \"--num_beams\", \"1\",\n","    \"--top_k\", \"50\",\n","]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["@dataclass\n","class ModelArguments:\n","    test_file: str\n","    model_name_or_path: str\n","    peft_model: str\n","    seed: int = field(default = 42)\n","    num_video_per_channel: int = field(default=1)\n","    test_size: Optional[int] = field(default=None)\n","    output_path: str = field(default='./prediction.json')\n","@dataclass\n","class TrainingArguments:\n","    batch_size: int = field(default=2)\n","    source_max_len: int = field(default=1024)\n","    target_max_len: int = field(default=256)\n","    \n","@dataclass\n","class GenerationArguments:\n","    max_new_tokens: int = field(default=256),\n","    min_new_tokens: int = field(default=None),\n","    do_sample: bool = field(default=False),\n","    num_beams: Optional[int] = field(default=1),\n","    num_beam_groups: Optional[int] = field(default=1)\n","    temperature: Optional[float] = field(default=None)\n","    top_k: Optional[int] = field(default=None)\n","    top_p: Optional[float] = field(default=None)\n","# Parser\n","def parse_generation_args(str_args = None):\n","    '''\n","    There is something buggy using dataclass for generation config. Use standard parser to parse.\n","    Error Message: \"TypeError: cannot pickle 'mappingproxy' object\" \n","    '''\n","    parser = argparse.ArgumentParser()\n","    # Generation Argument\n","    parser.add_argument(\n","        \"--max_new_tokens\",\n","        type=int,\n","        default=256\n","    )\n","    parser.add_argument(\n","        \"--min_new_tokens\",\n","        type=int,\n","        default=None\n","    )\n","    parser.add_argument(\n","        \"--do_sample\",\n","        action='store_true'\n","    )\n","    parser.add_argument(\n","        \"--num_beams\",\n","        type=int,\n","        default=1\n","    )\n","    parser.add_argument(\n","        \"--num_beam_groups\",\n","        type=int,\n","        default=1\n","    )\n","    parser.add_argument(\n","        \"--temperature\",\n","        type=float,\n","        default=None\n","    )\n","    parser.add_argument(\n","        \"--top_k\",\n","        type=int,\n","        default=None\n","    )\n","    parser.add_argument(\n","        \"--top_p\",\n","        type=float,\n","        default=None\n","    )\n","\n","    args = parser.parse_args(str_args)\n","    return args"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"C-u-ZOGlyzOP"},"outputs":[],"source":["@dataclass\n","class DataCollatorForCausalLM(object):\n","    tokenizer: transformers.PreTrainedTokenizer\n","    source_max_len: int\n","\n","    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n","        # Extract elements\n","        IGNORE_INDEX = -100\n","        sources = [f\"{self.tokenizer.bos_token}{example['input']}\" for example in instances]\n","        # Tokenize\n","        tokenized_sources_with_prompt = self.tokenizer(\n","            sources,\n","            max_length=self.source_max_len,\n","            truncation=True,\n","            add_special_tokens=False,\n","        )\n","        # Build the input for causal LM\n","        input_ids = []\n","        for tokenized_source in tokenized_sources_with_prompt['input_ids']:\n","                input_ids.append(torch.tensor(tokenized_source))\n","        # Apply padding\n","        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n","        data_dict = {\n","            'input_ids': input_ids,\n","            'attention_mask':input_ids.ne(self.tokenizer.pad_token_id),\n","        }\n","        return data_dict"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def get_bnb_config() -> BitsAndBytesConfig:\n","    '''Get the BitsAndBytesConfig.'''\n","    bnb_config = BitsAndBytesConfig(\n","            load_in_4bit= True,\n","            llm_int8_threshold=6.0,\n","            llm_int8_has_fp16_weight=False,\n","            bnb_4bit_compute_dtype=torch.float16,\n","            bnb_4bit_use_double_quant=False,\n","            bnb_4bit_quant_type=\"nf4\"\n","    )\n","    return bnb_config\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"fJbQxjbOzLLG"},"outputs":[],"source":["def main(str_args = None):\n","    pass\n","\n","hfparser = transformers.HfArgumentParser((\n","    ModelArguments, TrainingArguments\n","))\n","model_args, training_args, extra_args = \\\n","    hfparser.parse_args_into_dataclasses(str_args,return_remaining_strings=True)\n","generation_args = parse_generation_args(extra_args)\n","args = argparse.Namespace(\n","    **vars(model_args), **vars(training_args), **vars(generation_args)\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"2bIBeA_SyT_f"},"outputs":[],"source":["# Prepare\n","logger = logging.getLogger(__name__)\n","compute_dtype = torch.float16\n","if args.seed is not None:\n","    set_seed(args.seed)\n","if args.output_path is not None:\n","    output_dir = os.path.join(*args.output_path.split(\"/\")[:-1])\n","    os.makedirs(output_dir, exist_ok=True)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"avv5Xjdxzin_"},"outputs":[{"name":"stdout","output_type":"stream","text":["Load Dataset\n"]},{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 50/50 [00:00<00:00, 5661.09 examples/s]\n"]}],"source":["# Load dataset\n","print('Load Dataset')\n","def format_dataset(dataset):\n","    def processing(example):\n","        return {'input': get_prompt(example['video_title'], example['video_description'], example['star_num'], example['mood']),\n","                'output': example['comment_text']}\n","    formatted_dataset = dataset.map(processing)\n","    # Remove unused columns.\n","    formatted_dataset = formatted_dataset.remove_columns(\n","        [col for col in dataset.column_names if col in ['output']]\n","    )\n","    return formatted_dataset\n","\n","raw_dataset = prepare_dataset(args.test_file, num_video_per_channel=None, select=False, seed=args.seed)\n","if args.test_size is not None and raw_dataset.shape[0] > args.test_size:\n","    raw_dataset = raw_dataset.select(range(args.test_size))\n","test_dataset = format_dataset(raw_dataset)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"d-26dPvMpPiz"},"outputs":[{"name":"stdout","output_type":"stream","text":["Load Model\n"]},{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.68s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Loading adapters.\n"]}],"source":["# Load Model\n","print('Load Model')\n","bnb_config = get_bnb_config()\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    args.model_name_or_path,\n","    quantization_config = bnb_config,\n","    load_in_4bit = True,\n","    torch_dtype=compute_dtype,\n","    device_map = 'cuda:0'\n",")\n","base_model.config.torch_dtype=compute_dtype\n","# Load PeftModel\n","print(\"Loading adapters.\")\n","model = PeftModel.from_pretrained(base_model, args.peft_model)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"XCV0lOsF0pa3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Load Tokenizer\n"]},{"data":{"text/plain":["0"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Load Tokenizer\n","print('Load Tokenizer')\n","tokenizer = AutoTokenizer.from_pretrained(\n","    args.model_name_or_path,\n","    padding_side=\"right\",\n","    use_fast=False,\n","    tokenizer_type='llama'\n",")\n","tokenizer.add_special_tokens({\n","    \"eos_token\": tokenizer.convert_ids_to_tokens(base_model.config.eos_token_id),\n","    \"bos_token\": tokenizer.convert_ids_to_tokens(base_model.config.bos_token_id),\n","    \"unk_token\": tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id),\n","})"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"gctkZeYl1Dip"},"outputs":[],"source":["# Data Collator\n","data_collator = DataCollatorForCausalLM(\n","    tokenizer=tokenizer,\n","    source_max_len=args.source_max_len\n",")\n","# Generatrion Config\n","gen_config = transformers.GenerationConfig(\n","    max_new_tokens = args.max_new_tokens,\n","    min_new_tokens = args.min_new_tokens,\n","    do_sample = args.do_sample,\n","    num_beams = args.num_beams,\n","    num_beam_groups = args.num_beam_groups,\n","    top_k = args.top_k,\n","    top_p = args.top_p,\n","    temperature = args.temperature,\n","    )\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"hhfJi7Fr2cxn"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/13 [00:00<?, ?it/s]/home/vr/anaconda3/envs/adl-hw3-copy/lib/python3.10/site-packages/transformers/generation/utils.py:1539: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n","  warnings.warn(\n","100%|██████████| 13/13 [02:01<00:00, 10.17s/it]"]}],"source":["test_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=args.batch_size)\n","progress = tqdm(total=len(test_dataloader))\n","model.eval()\n","all_predictions=[]\n","for step, batch in enumerate(test_dataloader):\n","    with torch.no_grad():\n","        predictions = model.generate(\n","            input_ids=batch[\"input_ids\"],\n","            attention_mask=batch[\"attention_mask\"],\n","            generation_config = gen_config,\n","        )\n","        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n","        predictions = tokenizer.batch_decode(\n","            predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n","        )\n","        all_predictions += predictions\n","        progress.update()\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["with open(args.output_path, 'w') as fout:\n","    outputs = []\n","    for i, example in enumerate(test_dataset):\n","        output_example = {}\n","        output_example['cid'] = example['cid']\n","        output_example['mood'] = example['mood']\n","        output_example['output'] = all_predictions[i].replace(example['input'], '').strip()\n","        outputs.append(output_example)\n","    fout.write(json.dumps(outputs,indent=4,ensure_ascii=False))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNlr2j/ArHRqpE4KyIfpPQQ","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
