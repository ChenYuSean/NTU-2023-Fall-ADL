{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\adl-hw3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import transformers\n",
    "import argparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_files(path) -> pd.DataFrame:\n",
    "    '''\n",
    "    Returns a dataframe of root directory, file names, and content(list) of json files\n",
    "    '''\n",
    "    roots = []\n",
    "    file_names = []\n",
    "    contents = []\n",
    "    # Fast return if the path is a file\n",
    "    if path.endswith('.json'):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            content = json.load(f)\n",
    "        for record in content:\n",
    "            if type(record['votes']) == str:\n",
    "                if '萬' in record['votes']:\n",
    "                    record['votes'] = int(record['votes'].replace('萬', '')) * 10000\n",
    "        return pd.DataFrame({'root': [path], 'file_name': [path.split('/')[-1]], 'content': [content]})\n",
    "    \n",
    "    # Get the root, file names, and content\n",
    "    for root, dirnames, filenames in os.walk(path):\n",
    "        for file in filenames:\n",
    "            if file.endswith('.json') and file != \"star_record.json\":\n",
    "                with open(os.path.join(root,file), 'r', encoding='utf-8') as f:\n",
    "                    content = json.load(f)\n",
    "                for record in content:\n",
    "                    if type(record['votes']) == str:\n",
    "                        if '萬' in record['votes']:\n",
    "                            record['votes'] = int(record['votes'].replace('萬', '')) * 10000\n",
    "                roots.append(root)\n",
    "                file_names.append(file)\n",
    "                contents.append(content)\n",
    "    return pd.DataFrame({'root': roots, 'file_name': file_names, 'content': contents})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_clean(file_info: pd.DataFrame, num_video_per_channel = None, seed = None) -> pd.DataFrame :\n",
    "    '''\n",
    "    Randomly select data from each file and return a new dataframe of training data\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    moods = ['like','happiness','sadness','anger','fear','surprise','disgust']\n",
    "    \n",
    "    dataset_df = pd.DataFrame(columns = file_info.at[0,'content'][0].keys())\n",
    "    channels = file_info['root'].unique()\n",
    "    for channel in channels:\n",
    "        videos = file_info.loc[file_info['root'] == channel, ['file_name','content']]\n",
    "        if num_video_per_channel is not None:\n",
    "            videos = videos.sample(n = num_video_per_channel, random_state = seed).reset_index(drop=True)\n",
    "        for vid in videos.index:\n",
    "            content = pd.DataFrame(videos.loc[vid,'content'])\n",
    "            for mood in moods:\n",
    "                if  content.loc[content['mood'] == mood].size < 1: \n",
    "                    continue\n",
    "                pick_data = content.loc[content['mood'] == mood].sample(n = 1, random_state = None).reset_index(drop=True)\n",
    "                dataset_df = pd.concat([dataset_df,pick_data], ignore_index=True)\n",
    "    return dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(path, num_video_per_channel = None, seed = None):\n",
    "    '''\n",
    "    Returns a dataset of json files\n",
    "    '''\n",
    "    file_info = read_json_files(path)\n",
    "    datalist = data_clean(file_info, num_video_per_channel, seed=seed)\n",
    "    return Dataset.from_pandas(datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(title:str, description:str, star_num:str, mood:str) -> str:\n",
    "    '''Format the instruction as a prompt for LLM.'''\n",
    "\n",
    "    comment_type = '正面評論' if star_num.split('.')[0] in ['4', '5'] else '負面評論' if star_num.split('.')[0] in ['1', '2'] else '中立評論'\n",
    "    moods = ['like','happiness','sadness','anger','fear','surprise','disgust']\n",
    "    ch_moods = ['喜歡','開心','難過','生氣','害怕','驚訝','厭惡']\n",
    "    if mood in moods:\n",
    "        mood = ch_moods[moods.index(mood)]\n",
    "    \n",
    "    return f\"你是人工智慧助理，以下是用戶和人工智能助理之間的對話。你要對用戶的問題提供有用、詳細的回答。\\\n",
    "USER: 請幫這部影片生出對應需求的{comment_type}。影片標題:[{title}]。影片敘述:[{description}]。需求情感:[{mood}]。\\\n",
    "ASSISTANT:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['video_id', 'video_title', 'video_description', 'cid', 'comment_text', 'votes', 'time', 'star_num', 'mood'],\n",
      "    num_rows: 151\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanc\\AppData\\Local\\Temp\\ipykernel_13316\\3370045030.py:20: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataset_df = pd.concat([dataset_df,pick_data], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    PATH = \"./data/train_data/\"\n",
    "    info = read_json_files(PATH)\n",
    "    data = data_clean(info, num_video_per_channel=4)\n",
    "    print(Dataset.from_pandas(data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adl-hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
