{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "# import ipdb\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from itertools import chain\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from accelerate import notebook_launcher\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    MT5Model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "USE_NOTEBOOK_LAUNCHER = False\n",
    "str_args = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out when using .py file\n",
    "str_args = [\n",
    "    \"--train_file\", \"./data/train.jsonl\",\n",
    "    \"--output_dir\", \"./output\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(str_args = None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train_file\", type=str)\n",
    "    parser.add_argument(\"--validation_file\", type=str)\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=5e-5,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_train_epochs\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\", \n",
    "        type=str, \n",
    "        default=\"./output\"\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", type=int, default=None)\n",
    "    parser.add_argument(\"--checkpoint_step\", type=str, default=None)\n",
    "\n",
    "    \n",
    "    args = parser.parse_args(str_args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(str_args = None):\n",
    "    args = parse_args(str_args)\n",
    "    \n",
    "    model_name = \"google/mt5-small\"\n",
    "    # Prepare \n",
    "    if args.seed is not None:\n",
    "        set_seed(args.seed)\n",
    "    if args.output_dir is not None:\n",
    "        os.makedirs(args.output_dir, exist_ok=True)\n",
    "    \n",
    "        \n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps)\n",
    "    \n",
    "    # Load Dataset\n",
    "    data_files = {}\n",
    "    if args.train_file is not None:\n",
    "        data_files[\"train\"] = args.train_file\n",
    "    if args.validation_file is not None:\n",
    "        data_files[\"validation\"] = args.validation_file\n",
    "     \n",
    "    # Load Model\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = MT5Model.from_pretrained(model_name,config=config)\n",
    "\n",
    "    starting_epoch = 0\n",
    "    for epoch in range(starting_epoch, args.num_train_epochs):\n",
    "        model.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "d:\\anaconda3\\envs\\ADL2\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if USE_NOTEBOOK_LAUNCHER:\n",
    "        notebook_launcher(main,(str_args,), num_processes=1)\n",
    "    else:      \n",
    "        main(str_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39meval\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtw_rouge\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtw_rouge\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtwrouge\u001b[39;00m \u001b[39mimport\u001b[39;00m get_rouge\n",
      "File \u001b[1;32md:\\Course\\ADL\\HW2\\eval\\tw_rouge\\tw_rouge\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mtwrouge\u001b[39;00m \u001b[39mimport\u001b[39;00m get_rouge\n",
      "File \u001b[1;32md:\\Course\\ADL\\HW2\\eval\\tw_rouge\\tw_rouge\\twrouge.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mckiptagger\u001b[39;00m \u001b[39mimport\u001b[39;00m WS, data_utils\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mrouge\u001b[39;00m \u001b[39mimport\u001b[39;00m Rouge\n\u001b[1;32m----> 6\u001b[0m cache_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mXDG_CACHE_HOME\u001b[39m\u001b[39m\"\u001b[39m, os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(os\u001b[39m.\u001b[39;49mgetenv(\u001b[39m\"\u001b[39;49m\u001b[39mHOME\u001b[39;49m\u001b[39m\"\u001b[39;49m), \u001b[39m\"\u001b[39;49m\u001b[39m.cache\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m      7\u001b[0m download_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(cache_dir, \u001b[39m\"\u001b[39m\u001b[39mckiptagger\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m data_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(cache_dir, \u001b[39m\"\u001b[39m\u001b[39mckiptagger/data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\ADL2\\lib\\ntpath.py:78\u001b[0m, in \u001b[0;36mjoin\u001b[1;34m(path, *paths)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mjoin\u001b[39m(path, \u001b[39m*\u001b[39mpaths):\n\u001b[1;32m---> 78\u001b[0m     path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mfspath(path)\n\u001b[0;32m     79\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mbytes\u001b[39m):\n\u001b[0;32m     80\u001b[0m         sep \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "from twrouge import get_rouge"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('ADL2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ac1cd648590d730256148fc0cb16b46b58629ddaa66422a18e86bc8f5213d91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
