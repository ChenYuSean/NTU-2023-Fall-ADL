{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "\n",
    "# debug\n",
    "# import ipdb\n",
    "#\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import datasets\n",
    "import evaluate\n",
    "import torch\n",
    "import nltk\n",
    "import numpy as np\n",
    "import csv\n",
    "import json\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from accelerate import notebook_launcher\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "USE_NOTEBOOK_LAUNCHER = True\n",
    "str_args = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out when using .py file\n",
    "str_args = [\n",
    "    \"--train_file\", \"./data/train.jsonl\",\n",
    "    \"--batch_size\", \"1\",\n",
    "    \"--gradient_accumulation_steps\", \"2\",\n",
    "    \"--source_prefix\", \"\",\n",
    "    \"--max_source_length\", \"1024\", \n",
    "    \"--max_target_length\", \"128\",\n",
    "    \"--num_beams\", \"5\",\n",
    "    \"--num_train_epochs\", \"10\",\n",
    "    \"--checkpointing_steps\", \"2500\",\n",
    "    \"--output_dir\", \"./output\"    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(str_args = None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Data\n",
    "    parser.add_argument(\"--seed\", type=int, default=None)\n",
    "    parser.add_argument(\"--train_file\", type=str ,required=True)\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\", \n",
    "        type=str, \n",
    "        default=\"./output\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--split_ratio\",\n",
    "        type = float,\n",
    "        default= 0.1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        type=str,\n",
    "        default = \"google/mt5-small\"\n",
    "    )\n",
    "    # Training Parameters\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=5e-5,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_train_epochs\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_train_steps\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_beams\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "    )\n",
    "    # Preprocessing\n",
    "    parser.add_argument(\n",
    "        \"--source_prefix\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_source_length\",\n",
    "        type=int,\n",
    "        default=1024,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_target_length\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "    )    \n",
    "    parser.add_argument(\n",
    "        \"--preprocessing_num_workers\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "    )\n",
    "    \n",
    "    # Checkpoint\n",
    "    parser.add_argument(\n",
    "        \"--checkpointing_steps\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_from_checkpoint\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"If the training should continue from a checkpoint folder.\",\n",
    "    )\n",
    "\n",
    "    \n",
    "    args = parser.parse_args(str_args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(str_args = None):\n",
    "    args = parse_args(str_args)\n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        mixed_precision = None\n",
    "    )\n",
    "    \n",
    "    # Prepare \n",
    "    if args.seed is not None:\n",
    "        set_seed(args.seed)\n",
    "        \n",
    "    if accelerator.is_main_process: \n",
    "        if args.output_dir is not None:\n",
    "            os.makedirs(args.output_dir, exist_ok=True)\n",
    "    accelerator.wait_for_everyone()\n",
    "        \n",
    "    # Load Dataset\n",
    "    split = load_dataset(\"json\", data_files=args.train_file,split='train').train_test_split(test_size=args.split_ratio)\n",
    "    raw_datasets = DatasetDict({'train': split['train'], 'validation': split['test']})\n",
    "    del split\n",
    "    # Load Model\n",
    "    config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "    # We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "    # on a small vocab and want a smaller embedding size, remove this test.\n",
    "    embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "    if len(tokenizer) > embedding_size:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    if model.config.decoder_start_token_id is None:\n",
    "        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "    \n",
    "    prefix = args.source_prefix if args.source_prefix is not None else \"\"\n",
    "    # Preprocessing the datasets.\n",
    "    # First we tokenize all the texts.    \n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "    text_column = 'maintext'\n",
    "    summary_column = 'title'\n",
    "    \n",
    "    max_target_length = args.max_target_length\n",
    "    padding = False\n",
    "    def preprocess_function(examples):\n",
    "        inputs = examples[text_column]\n",
    "        targets = examples[summary_column]\n",
    "        inputs = [prefix + inp for inp in inputs]\n",
    "        model_inputs = tokenizer(inputs, max_length=args.max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "        # Tokenize targets with the `text_target` keyword argument\n",
    "        labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    with accelerator.main_process_first():\n",
    "        train_dataset = raw_datasets[\"train\"].map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "        )\n",
    "        eval_dataset = raw_datasets[\"validation\"].map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "        )\n",
    "\n",
    "    # Data Collator\n",
    "    label_pad_token_id = -100\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of = 8 if accelerator.use_fp16 else None,\n",
    "    )\n",
    "\n",
    "    # Postprocessing the predictions\n",
    "    def postprocess_text(preds, labels):\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        labels = [label.strip() for label in labels]\n",
    "\n",
    "        # rougeLSum expects newline after each sentence\n",
    "        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "        return preds, labels\n",
    "    \n",
    "    # Data Loader\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.batch_size\n",
    "    )\n",
    "    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.batch_size)\n",
    "    \n",
    "    # Optimizer\n",
    "    # Split weights in two groups, one with weight decay and the other not.\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "\n",
    "    # Scheduler and math around the number of training steps.\n",
    "    overrode_max_train_steps = False\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    if args.max_train_steps is None:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "        overrode_max_train_steps = True\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n",
    "    )\n",
    "    \n",
    "    # Prepare everything with our `accelerator`.\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    "    )\n",
    "    \n",
    "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    if overrode_max_train_steps:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    # Afterwards we recalculate our number of training epochs\n",
    "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    # Figure out how many steps we should save the Accelerator states\n",
    "    checkpointing_steps = args.checkpointing_steps\n",
    "    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n",
    "        checkpointing_steps = int(checkpointing_steps)\n",
    "        \n",
    "    # Metric\n",
    "    # Use the get_rouge frome tw_rouge\n",
    "    # metric = evaluate.load(\"rouge\")\n",
    "\n",
    "    # Train!\n",
    "    total_batch_size = args.batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    completed_steps = 0\n",
    "    starting_epoch = 0\n",
    "    all_results = []\n",
    "    # Potentially load in the weights and states from a previous save\n",
    "    if args.resume_from_checkpoint:\n",
    "        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n",
    "            checkpoint_path = args.resume_from_checkpoint\n",
    "            path = os.path.basename(args.resume_from_checkpoint)\n",
    "        else:\n",
    "            # Get the most recent checkpoint\n",
    "            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n",
    "            dirs.sort(key=os.path.getctime)\n",
    "            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n",
    "            checkpoint_path = path\n",
    "            path = os.path.basename(checkpoint_path)\n",
    "\n",
    "        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n",
    "        accelerator.load_state(checkpoint_path)\n",
    "        # Extract `epoch_{i}` or `step_{i}`\n",
    "        training_difference = os.path.splitext(path)[0]\n",
    "\n",
    "        if \"epoch\" in training_difference:\n",
    "            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n",
    "            resume_step = None\n",
    "            completed_steps = starting_epoch * num_update_steps_per_epoch\n",
    "        else:\n",
    "            # need to multiply `gradient_accumulation_steps` to reflect real steps\n",
    "            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n",
    "            starting_epoch = resume_step // len(train_dataloader)\n",
    "            completed_steps = resume_step // args.gradient_accumulation_steps\n",
    "            resume_step -= starting_epoch * len(train_dataloader)\n",
    "\n",
    "    # update the progress_bar if load from checkpoint\n",
    "    progress_bar.update(completed_steps)\n",
    "    \n",
    "    for epoch in range(starting_epoch, args.num_train_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        if args.resume_from_checkpoint and epoch == starting_epoch and resume_step is not None:\n",
    "            # We skip the first `n` batches in the dataloader when resuming from a checkpoint\n",
    "            active_dataloader = accelerator.skip_first_batches(train_dataloader, resume_step)\n",
    "        else:\n",
    "            active_dataloader = train_dataloader\n",
    "        for step, batch in enumerate(active_dataloader):\n",
    "            with accelerator.accumulate(model):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "                accelerator.backward(loss)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                completed_steps += 1\n",
    "\n",
    "            if isinstance(checkpointing_steps, int):\n",
    "                if completed_steps % checkpointing_steps == 0:\n",
    "                    output_dir = f\"step_{completed_steps}\"\n",
    "                    if args.output_dir is not None:\n",
    "                        output_dir = os.path.join(args.output_dir, output_dir)\n",
    "                    accelerator.save_state(output_dir)\n",
    "\n",
    "            if completed_steps >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        refs = []\n",
    "        gen_kwargs = {\n",
    "            \"max_length\": args.max_target_length,\n",
    "            \"num_beams\": args.num_beams,\n",
    "        }\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "                generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                    batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    **gen_kwargs,\n",
    "                )\n",
    "\n",
    "                generated_tokens = accelerator.pad_across_processes(\n",
    "                    generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "                )\n",
    "                labels = batch[\"labels\"]\n",
    "                labels = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "\n",
    "                generated_tokens, labels = accelerator.gather_for_metrics((generated_tokens, labels))\n",
    "                generated_tokens = generated_tokens.cpu().numpy()\n",
    "                labels = labels.cpu().numpy()\n",
    "\n",
    "                labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "                if isinstance(generated_tokens, tuple):\n",
    "                    generated_tokens = generated_tokens[0]\n",
    "                decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "                decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "                decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "                preds += decoded_preds\n",
    "                refs += decoded_labels\n",
    "                \n",
    "        result = {\"pred\":preds,\"ref\":refs}\n",
    "        all_results.append(result)\n",
    "    if args.output_dir is not None:\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        unwrapped_model.save_pretrained(\n",
    "            args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "        )\n",
    "        if accelerator.is_main_process:\n",
    "            tokenizer.save_pretrained(args.output_dir)\n",
    "            \n",
    "            # Save the prediction\n",
    "            with open(os.path.join(args.output_dir,\"valid_result.json\"), \"w\") as f:\n",
    "                f.write(json.dumps(all_results,indent = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 2110.87it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 383.36it/s]\n",
      "Generating train split: 21710 examples [00:00, 47046.05 examples/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/vr/anaconda3/envs/adl2-copy/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 19539/19539 [00:10<00:00, 1948.18 examples/s]\n",
      "Map: 100%|██████████| 2171/2171 [00:01<00:00, 1973.66 examples/s]\n",
      "/home/vr/anaconda3/envs/adl2-copy/lib/python3.9/site-packages/accelerate/accelerator.py:523: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/97700 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|          | 100/97700 [00:20<5:13:50,  5.18it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/vr/disk/YuSean/ADL/HW2/train_t5.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vr/disk/YuSean/ADL/HW2/train_t5.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     notebook_launcher(main,(str_args,), num_processes\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/vr/disk/YuSean/ADL/HW2/train_t5.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39melse\u001b[39;00m:      \n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/vr/disk/YuSean/ADL/HW2/train_t5.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     main(str_args)\n",
      "\u001b[1;32m/home/vr/disk/YuSean/ADL/HW2/train_t5.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/vr/disk/YuSean/ADL/HW2/train_t5.ipynb#W5sZmlsZQ%3D%3D?line=194'>195</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/vr/disk/YuSean/ADL/HW2/train_t5.ipynb#W5sZmlsZQ%3D%3D?line=195'>196</a>\u001b[0m     active_dataloader \u001b[39m=\u001b[39m train_dataloader\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/vr/disk/YuSean/ADL/HW2/train_t5.ipynb#W5sZmlsZQ%3D%3D?line=196'>197</a>\u001b[0m \u001b[39mfor\u001b[39;00m step, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(active_dataloader):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/vr/disk/YuSean/ADL/HW2/train_t5.ipynb#W5sZmlsZQ%3D%3D?line=197'>198</a>\u001b[0m     \u001b[39mwith\u001b[39;00m accelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/vr/disk/YuSean/ADL/HW2/train_t5.ipynb#W5sZmlsZQ%3D%3D?line=198'>199</a>\u001b[0m         outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mbatch)\n",
      "File \u001b[0;32m~/anaconda3/envs/adl2-copy/lib/python3.9/site-packages/accelerate/data_loader.py:393\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     \u001b[39m# But we still move it to the device so it is done before `StopIteration` is reached\u001b[39;00m\n\u001b[1;32m    392\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 393\u001b[0m         current_batch \u001b[39m=\u001b[39m send_to_device(current_batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    394\u001b[0m     next_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataloader_iter)\n\u001b[1;32m    395\u001b[0m     \u001b[39mif\u001b[39;00m batch_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_batches:\n",
      "File \u001b[0;32m~/anaconda3/envs/adl2-copy/lib/python3.9/site-packages/accelerate/utils/operations.py:160\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[39melif\u001b[39;00m skip_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         skip_keys \u001b[39m=\u001b[39m []\n\u001b[1;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensor)(\n\u001b[0;32m--> 160\u001b[0m         {\n\u001b[1;32m    161\u001b[0m             k: t \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m skip_keys \u001b[39melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[39m=\u001b[39mnon_blocking, skip_keys\u001b[39m=\u001b[39mskip_keys)\n\u001b[1;32m    162\u001b[0m             \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensor\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    163\u001b[0m         }\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/adl2-copy/lib/python3.9/site-packages/accelerate/utils/operations.py:161\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[39melif\u001b[39;00m skip_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         skip_keys \u001b[39m=\u001b[39m []\n\u001b[1;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensor)(\n\u001b[1;32m    160\u001b[0m         {\n\u001b[0;32m--> 161\u001b[0m             k: t \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m skip_keys \u001b[39melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking, skip_keys\u001b[39m=\u001b[39;49mskip_keys)\n\u001b[1;32m    162\u001b[0m             \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensor\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    163\u001b[0m         }\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/adl2-copy/lib/python3.9/site-packages/accelerate/utils/operations.py:167\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mto(device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking)\n\u001b[1;32m    168\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    169\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if USE_NOTEBOOK_LAUNCHER:\n",
    "        notebook_launcher(main,(str_args,), num_processes=2)\n",
    "    else:      \n",
    "        main(str_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('ADL2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ac1cd648590d730256148fc0cb16b46b58629ddaa66422a18e86bc8f5213d91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
