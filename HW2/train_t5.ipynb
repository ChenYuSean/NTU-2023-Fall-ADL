{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\ADL2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "# import ipdb\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import datasets\n",
    "import evaluate\n",
    "import torch\n",
    "import nltk\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from accelerate import notebook_launcher\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "USE_NOTEBOOK_LAUNCHER = False\n",
    "str_args = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out when using .py file\n",
    "str_args = [\n",
    "    \"--train_file\", \"./data/train_example.jsonl\",\n",
    "    \"--output_dir\", \"./output\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_args(str_args = None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Data\n",
    "    parser.add_argument(\"--seed\", type=int, default=None)\n",
    "    parser.add_argument(\"--train_file\", type=str ,required=True)\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\", \n",
    "        type=str, \n",
    "        default=\"./output\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--split_ratio\",\n",
    "        type = float,\n",
    "        default= 0.1\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        type=str,\n",
    "        default = \"google/mt5-small\"\n",
    "    )\n",
    "    # Training Parameters\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=5e-5,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_train_epochs\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_train_steps\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "    )\n",
    "    # Preprocessing\n",
    "    parser.add_argument(\n",
    "        \"--source_prefix\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_source_length\",\n",
    "        type=int,\n",
    "        default=1024,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_target_length\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "    )    \n",
    "    parser.add_argument(\n",
    "        \"--preprocessing_num_workers\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "    )\n",
    "    \n",
    "    # Checkpoint\n",
    "    parser.add_argument(\n",
    "        \"--checkpointing_steps\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_from_checkpoint\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"If the training should continue from a checkpoint folder.\",\n",
    "    )\n",
    "\n",
    "    \n",
    "    args = parser.parse_args(str_args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(str_args = None):\n",
    "    args = parse_args(str_args)\n",
    "    \n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps)\n",
    "    \n",
    "    # Prepare \n",
    "    if args.seed is not None:\n",
    "        set_seed(args.seed)\n",
    "        \n",
    "    if accelerator.is_main_process: \n",
    "        if args.output_dir is not None:\n",
    "            os.makedirs(args.output_dir, exist_ok=True)\n",
    "    accelerator.wait_for_everyone()\n",
    "        \n",
    "    # Load Dataset\n",
    "    split = load_dataset(\"json\", data_files=args.train_file,split='train').train_test_split(test_size=args.split_ratio)\n",
    "    raw_datasets = DatasetDict({'train': split['train'], 'validation': split['test']})\n",
    "    \n",
    "    # Load Model\n",
    "    config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "    # We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "    # on a small vocab and want a smaller embedding size, remove this test.\n",
    "    embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "    if len(tokenizer) > embedding_size:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    if model.config.decoder_start_token_id is None:\n",
    "        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "    \n",
    "    prefix = args.source_prefix if args.source_prefix is not None else \"\"\n",
    "    # Preprocessing the datasets.\n",
    "    # First we tokenize all the texts.    \n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "    text_column = 'maintext'\n",
    "    summary_column = 'title'\n",
    "    \n",
    "    max_target_length = args.max_target_length\n",
    "    padding = False\n",
    "    def preprocess_function(examples):\n",
    "        inputs = examples[text_column]\n",
    "        targets = examples[summary_column]\n",
    "        inputs = [prefix + inp for inp in inputs]\n",
    "        model_inputs = tokenizer(inputs, max_length=args.max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "        # Tokenize targets with the `text_target` keyword argument\n",
    "        labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    with accelerator.main_process_first():\n",
    "        train_dataset = raw_datasets[\"train\"].map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "        )\n",
    "        eval_dataset = raw_datasets[\"validation\"].map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "        )\n",
    "\n",
    "    # Data Collator\n",
    "    label_pad_token_id = -100\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of = None\n",
    "    )\n",
    "\n",
    "    # Postprocessing the predictions\n",
    "    def postprocess_text(preds, labels):\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        labels = [label.strip() for label in labels]\n",
    "\n",
    "        # rougeLSum expects newline after each sentence\n",
    "        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "        return preds, labels\n",
    "    \n",
    "    # Data Loader\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.batch_size\n",
    "    )\n",
    "    eval_dataloader = DataLoader(eval_dataset, collate_fn=data_collator, batch_size=args.batch_size)\n",
    "    \n",
    "    # Optimizer\n",
    "    # Split weights in two groups, one with weight decay and the other not.\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "\n",
    "    # Scheduler and math around the number of training steps.\n",
    "    overrode_max_train_steps = False\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    if args.max_train_steps is None:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "        overrode_max_train_steps = True\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n",
    "    )\n",
    "    \n",
    "    # Prepare everything with our `accelerator`.\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    "    )\n",
    "    \n",
    "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    if overrode_max_train_steps:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    # Afterwards we recalculate our number of training epochs\n",
    "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    # Figure out how many steps we should save the Accelerator states\n",
    "    checkpointing_steps = args.checkpointing_steps\n",
    "    if checkpointing_steps is not None and checkpointing_steps.isdigit():\n",
    "        checkpointing_steps = int(checkpointing_steps)\n",
    "        \n",
    "    # Metric\n",
    "    # metric = evaluate.load(\"rouge\")\n",
    "\n",
    "    # Train!\n",
    "    total_batch_size = args.batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(range(args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    completed_steps = 0\n",
    "    starting_epoch = 0\n",
    "    \n",
    "    # Potentially load in the weights and states from a previous save\n",
    "    if args.resume_from_checkpoint:\n",
    "        if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n",
    "            checkpoint_path = args.resume_from_checkpoint\n",
    "            path = os.path.basename(args.resume_from_checkpoint)\n",
    "        else:\n",
    "            # Get the most recent checkpoint\n",
    "            dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n",
    "            dirs.sort(key=os.path.getctime)\n",
    "            path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n",
    "            checkpoint_path = path\n",
    "            path = os.path.basename(checkpoint_path)\n",
    "\n",
    "        accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n",
    "        accelerator.load_state(checkpoint_path)\n",
    "        # Extract `epoch_{i}` or `step_{i}`\n",
    "        training_difference = os.path.splitext(path)[0]\n",
    "\n",
    "        if \"epoch\" in training_difference:\n",
    "            starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n",
    "            resume_step = None\n",
    "            completed_steps = starting_epoch * num_update_steps_per_epoch\n",
    "        else:\n",
    "            # need to multiply `gradient_accumulation_steps` to reflect real steps\n",
    "            resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n",
    "            starting_epoch = resume_step // len(train_dataloader)\n",
    "            completed_steps = resume_step // args.gradient_accumulation_steps\n",
    "            resume_step -= starting_epoch * len(train_dataloader)\n",
    "\n",
    "    # update the progress_bar if load from checkpoint\n",
    "    progress_bar.update(completed_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "d:\\anaconda3\\envs\\ADL2\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 18/18 [00:00<00:00, 396.53 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 159.03 examples/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if USE_NOTEBOOK_LAUNCHER:\n",
    "        notebook_launcher(main,(str_args,), num_processes=1)\n",
    "    else:      \n",
    "        main(str_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('ADL2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ac1cd648590d730256148fc0cb16b46b58629ddaa66422a18e86bc8f5213d91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
