{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "# import ipdb\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import datasets\n",
    "import evaluate\n",
    "import torch\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from accelerate import notebook_launcher\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    SchedulerType,\n",
    "    get_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "USE_NOTEBOOK_LAUNCHER = False\n",
    "str_args = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out when using .py file\n",
    "str_args = [\n",
    "    \"--test_file\", \"./data/public.jsonl\",\n",
    "    \"--output_dir\", \"./output\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(str_args = None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Data\n",
    "    parser.add_argument(\"--seed\", type=int, default=None)\n",
    "    parser.add_argument(\"--test_file\", type=str ,required=True)\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\", \n",
    "        type=str, \n",
    "        default=\"./output\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        type=str,\n",
    "        default = \"google/mt5-small\"\n",
    "    )\n",
    "    # Training Parameters\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        type=int,\n",
    "        default=8,\n",
    "    )\n",
    "    # Preprocessing\n",
    "    parser.add_argument(\n",
    "        \"--source_prefix\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_source_length\",\n",
    "        type=int,\n",
    "        default=1024,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_target_length\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "    )    \n",
    "    parser.add_argument(\n",
    "        \"--preprocessing_num_workers\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "    )\n",
    "\n",
    "    \n",
    "    args = parser.parse_args(str_args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(str_args = None):\n",
    "    args = parse_args(str_args)\n",
    "    \n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator(gradient_accumulation_steps=args.gradient_accumulation_steps)\n",
    "    \n",
    "    # Prepare \n",
    "    if args.seed is not None:\n",
    "        set_seed(args.seed)\n",
    "        \n",
    "    if accelerator.is_main_process: \n",
    "        if args.output_dir is not None:\n",
    "            os.makedirs(args.output_dir, exist_ok=True)\n",
    "    accelerator.wait_for_everyone()\n",
    "        \n",
    "    # Load Dataset\n",
    "    data_files ={}\n",
    "    data_files['test'] = args.test_file\n",
    "    raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "    \n",
    "    # Load Model\n",
    "    config = AutoConfig.from_pretrained(args.model_name_or_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            args.model_name_or_path,\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "    # We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "    # on a small vocab and want a smaller embedding size, remove this test.\n",
    "    embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "    if len(tokenizer) > embedding_size:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    if model.config.decoder_start_token_id is None:\n",
    "        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "    \n",
    "    prefix = args.source_prefix if args.source_prefix is not None else \"\"\n",
    "    # Preprocessing the datasets.\n",
    "    # First we tokenize all the texts.    \n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "    text_column = 'maintext'\n",
    "    summary_column = 'title'\n",
    "    \n",
    "    max_target_length = args.max_target_length\n",
    "    padding = False\n",
    "    def preprocess_function(examples):\n",
    "        inputs = examples[text_column]\n",
    "        targets = examples[summary_column]\n",
    "        inputs = [prefix + inp for inp in inputs]\n",
    "        model_inputs = tokenizer(inputs, max_length=args.max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "        # Tokenize targets with the `text_target` keyword argument\n",
    "        labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "    \n",
    "    with accelerator.main_process_first():\n",
    "        test_dataset = raw_datasets[\"test\"].map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "        )\n",
    "\n",
    "    # Data Collator\n",
    "    label_pad_token_id = -100\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of = None\n",
    "    )\n",
    "\n",
    "    # Postprocessing the predictions\n",
    "    def postprocess_text(preds, labels):\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        labels = [label.strip() for label in labels]\n",
    "\n",
    "        # rougeLSum expects newline after each sentence\n",
    "        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "        return preds, labels\n",
    "    \n",
    "    # Data Loader\n",
    "    \n",
    "    test_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=args.batch_size)\n",
    "    \n",
    "    # Prepare everything with our `accelerator`.\n",
    "    model,test_dataloader = accelerator.prepare(\n",
    "        model,test_dataloader\n",
    "    )\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    refs = []\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": args.max_target_length,\n",
    "        \"num_beams\": args.num_beams,\n",
    "    }\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                **gen_kwargs,\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "            labels = batch[\"labels\"]\n",
    "            labels = accelerator.pad_across_processes(batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id)\n",
    "\n",
    "            generated_tokens, labels = accelerator.gather_for_metrics((generated_tokens, labels))\n",
    "            generated_tokens = generated_tokens.cpu().numpy()\n",
    "            labels = labels.cpu().numpy()\n",
    "\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "            preds += decoded_preds\n",
    "            refs += decoded_labels\n",
    "            \n",
    "    result = get_rouge(preds,refs)\n",
    "    all_results.append(result)\n",
    "    \n",
    "    if args.output_dir is not None:\n",
    "        accelerator.wait_for_everyone()\n",
    "        # Save predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if USE_NOTEBOOK_LAUNCHER:\n",
    "        notebook_launcher(main,(str_args,), num_processes=1)\n",
    "    else:      \n",
    "        main(str_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a8dfe095fce2b5e88c64a2c3ee084c8e0e0d70b23e7b95b1cfb538be294c5c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
