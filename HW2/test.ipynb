{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vr/anaconda3/envs/adl2-copy/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-02 16:21:09.511882: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-02 16:21:09.511915: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-02 16:21:09.511935: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-02 16:21:09.518335: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-02 16:21:10.220267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "import ipdb\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import nltk\n",
    "import numpy as np\n",
    "import json\n",
    "import jsonlines\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from accelerate.utils import set_seed\n",
    "from accelerate import notebook_launcher\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "USE_NOTEBOOK_LAUNCHER = False\n",
    "MODEL_NAME = \"pytorch_model.bin\"\n",
    "CONFIG_NAME = \"config.json\"\n",
    "RESULTs = None\n",
    "str_args = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment out when using .py file\n",
    "str_args = [\n",
    "    \"--test_file\", \"./data/public_subset.jsonl\",\n",
    "    \"--model_name_or_path\", \"./output\",\n",
    "    \"--batch_size\", \"2\",\n",
    "    \"--num_beams\", \"5\",\n",
    "    # \"--top_k\", \"10\",\n",
    "    # \"--top_p\", \"0.9\",\n",
    "    # \"--temperature\", \"1.0\",\n",
    "    \"--source_prefix\", \"\",\n",
    "    \"--max_source_length\", \"1024\", \n",
    "    \"--max_target_length\", \"128\",\n",
    "    \"--output_path\", \"./OUTPUTS/Predictions/Beam_Top_p.jsonl\" \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(str_args = None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Data\n",
    "    parser.add_argument(\"--test_file\", type=str ,required=True)\n",
    "    parser.add_argument(\"--seed\", type=int, default=None)\n",
    "    parser.add_argument(\n",
    "        \"--output_path\", \n",
    "        type=str, \n",
    "        default=\"./prediction.jsonl\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_name_or_path\",\n",
    "        type=str,\n",
    "        default = \"google/mt5-small\"\n",
    "    )\n",
    "    # Predicting Parameters\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_beams\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--temperature\",\n",
    "        type=float,\n",
    "        default=None,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--top_k\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--top_p\",\n",
    "        type=float,\n",
    "        default=None,\n",
    "    )\n",
    "    # Preprocessing\n",
    "    parser.add_argument(\n",
    "        \"--source_prefix\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_source_length\",\n",
    "        type=int,\n",
    "        default=1024,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_target_length\",\n",
    "        type=int,\n",
    "        default=128,\n",
    "    )    \n",
    "    parser.add_argument(\n",
    "        \"--preprocessing_num_workers\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "    )\n",
    "\n",
    "    \n",
    "    args = parser.parse_args(str_args)\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(str_args = None):\n",
    "    args = parse_args(str_args)\n",
    "    \n",
    "    # Initialize accelerator\n",
    "    accelerator = Accelerator()\n",
    "    \n",
    "    # Prepare \n",
    "    if args.seed is not None:\n",
    "        set_seed(args.seed)\n",
    "        \n",
    "    if accelerator.is_main_process: \n",
    "        if args.output_path is not None:\n",
    "            os.makedirs(os.path.join(*args.output_path.split(\"/\")[:-1]), exist_ok=True)\n",
    "    accelerator.wait_for_everyone()\n",
    "        \n",
    "    # Load Dataset\n",
    "    data_files ={}\n",
    "    data_files['test'] = args.test_file\n",
    "    raw_datasets = load_dataset(\"json\", data_files=data_files)\n",
    "    \n",
    "    # Load Model\n",
    "    config = AutoConfig.from_pretrained(os.path.join(args.model_name_or_path,CONFIG_NAME))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, use_fast=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            os.path.join(args.model_name_or_path,MODEL_NAME),\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "    # We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "    # on a small vocab and want a smaller embedding size, remove this test.\n",
    "    embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "    if len(tokenizer) > embedding_size:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "    if model.config.decoder_start_token_id is None:\n",
    "        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n",
    "    \n",
    "    prefix = args.source_prefix if args.source_prefix is not None else \"\"\n",
    "    # Preprocessing the datasets.\n",
    "    # First we tokenize all the texts.    \n",
    "    column_names = raw_datasets[\"test\"].column_names\n",
    "    text_column = 'maintext'\n",
    "    \n",
    "    max_target_length = args.max_target_length\n",
    "    padding = False\n",
    "    def preprocess_function(examples):\n",
    "        inputs = examples[text_column]\n",
    "        inputs = [prefix + inp for inp in inputs]\n",
    "        model_inputs = tokenizer(inputs, max_length=args.max_source_length, padding=padding, truncation=True)\n",
    "        return model_inputs\n",
    "    \n",
    "    with accelerator.main_process_first():\n",
    "        test_dataset = raw_datasets[\"test\"].map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            num_proc=args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "        )\n",
    "\n",
    "    # Data Collator\n",
    "    label_pad_token_id = -100\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of = 8 if accelerator.use_fp16 else None,\n",
    "    )\n",
    "\n",
    "    # Postprocessing the predictions\n",
    "    def postprocess_text(preds):\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "\n",
    "        # rougeLSum expects newline after each sentence\n",
    "        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    # Data Loader\n",
    "    \n",
    "    test_dataloader = DataLoader(test_dataset, collate_fn=data_collator, batch_size=args.batch_size)\n",
    "    \n",
    "    # Prepare everything with our `accelerator`.\n",
    "    model,test_dataloader = accelerator.prepare(\n",
    "        model,test_dataloader\n",
    "    )\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    do_sample = True if args.top_k is not None or args.top_p is not None else False\n",
    "    gen_kwargs = {\n",
    "        \"max_length\": args.max_target_length,\n",
    "        \"num_beams\": args.num_beams,\n",
    "        \"top_k\" : args.top_k,\n",
    "        \"top_p\" : args.top_p,\n",
    "        \"temperature\" : args.temperature,\n",
    "        \"do_sample\": do_sample\n",
    "    }\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                **gen_kwargs,\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            generated_tokens = accelerator.gather_for_metrics(generated_tokens)\n",
    "            generated_tokens = generated_tokens.cpu().numpy()\n",
    "\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "            decoded_preds = postprocess_text(decoded_preds)\n",
    "            preds += decoded_preds\n",
    "    \n",
    "    if args.output_path is not None:\n",
    "        accelerator.wait_for_everyone()\n",
    "        # Save predictions\n",
    "        results = list(map(lambda pred,id: {'title': pred, 'id': id}, preds, raw_datasets['test']['id']))\n",
    "        with jsonlines.open(args.output_path,'w') as writer:\n",
    "            for result in results:\n",
    "                writer.write(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 1842.21 examples/s]\n",
      "/home/vr/anaconda3/envs/adl2-copy/lib/python3.9/site-packages/accelerate/accelerator.py:523: FutureWarning: The `use_fp16` property is deprecated and will be removed in version 1.0 of Accelerate use `Accelerator.mixed_precision == 'fp16'` instead.\n",
      "  warnings.warn(\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    if USE_NOTEBOOK_LAUNCHER:\n",
    "        notebook_launcher(main,(str_args,), num_processes=1)\n",
    "    else:      \n",
    "        main(str_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('ADL2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ac1cd648590d730256148fc0cb16b46b58629ddaa66422a18e86bc8f5213d91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
